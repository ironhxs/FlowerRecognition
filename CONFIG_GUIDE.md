# Training Configuration Quick Reference

## ğŸ“‹ ä½¿ç”¨æ–¹æ³•

### æ ¹æ®æ˜¾å¡é€‰æ‹©é…ç½®

```bash
# å°æ˜¾å¡ (8GB-12GB) - ConvNeXt Base
python train.py --config-name train_small_gpu

# ä¸­ç­‰æ˜¾å¡ (16GB-20GB) - EfficientNetV2-L  
python train.py --config-name train_medium_gpu

# å¤§æ˜¾å¡ (24GB+) - EfficientNetV2-L å…¨é€Ÿ
python train.py --config-name train_large_gpu

# å¿«é€Ÿæµ‹è¯• (éªŒè¯æµç¨‹)
python train.py --config-name train_quick_test
```

## ğŸ¯ Hydra é…ç½®ä¼˜å…ˆçº§ï¼ˆä»é«˜åˆ°ä½ï¼‰

1. **å‘½ä»¤è¡Œå‚æ•°** (æœ€é«˜ä¼˜å…ˆçº§)
   ```bash
   python train.py --config-name train_medium_gpu training.epochs=50
   ```

2. **--config-name æŒ‡å®šçš„é…ç½®æ–‡ä»¶**
   ```bash
   train_medium_gpu.yaml  # ä¼šè¦†ç›– config.yaml
   ```

3. **defaults åˆ—è¡¨ä¸­çš„é…ç½®**
   ```yaml
   defaults:
     - model: efficientnet_v2_l_optimized  # è¦†ç›–é»˜è®¤ model
     - training: focal                      # è¦†ç›–é»˜è®¤ training
   ```

4. **åŸºç¡€é…ç½®æ–‡ä»¶**
   ```bash
   configs/config.yaml  # æœ€ä½ä¼˜å…ˆçº§
   ```

## ğŸ”§ å¸¸è§å‚æ•°è¦†ç›–ç¤ºä¾‹

### åªæ”¹ batch size
```bash
python train.py --config-name train_medium_gpu dataset.batch_size=8
```

### æ”¹å¤šä¸ªå‚æ•°
```bash
python train.py --config-name train_medium_gpu \
  dataset.batch_size=12 \
  training.epochs=50 \
  training.optimizer.lr=0.00005
```

### æ¢æ¨¡å‹ä½†ä¿æŒå…¶ä»–è®¾ç½®
```bash
python train.py --config-name train_medium_gpu model=convnext_base
```

### å…³é—­æ•°æ®å¢å¼ºæµ‹è¯•
```bash
python train.py --config-name train_quick_test augmentation=light
```

## ğŸ“Š é…ç½®æ–‡ä»¶å¯¹æ¯”

| é…ç½®æ–‡ä»¶ | æ˜¾å­˜éœ€æ±‚ | æ¨¡å‹ | Batch Size | ç´¯ç§¯æ­¥æ•° | æœ‰æ•ˆBatch |
|---------|---------|------|-----------|---------|----------|
| train_small_gpu | 8-12GB | ConvNeXt Base | 8 | 4 | 32 |
| train_medium_gpu | 16-20GB | EfficientNetV2-L | 16 | 2 | 32 |
| train_large_gpu | 24GB+ | EfficientNetV2-L | 32 | 1 | 32 |
| train_quick_test | ä»»æ„ | ConvNeXt Tiny | 16 | 1 | 16 |

## ğŸ“ æ¢¯åº¦ç´¯ç§¯è¯´æ˜

**æœ‰æ•ˆ Batch Size = batch_size Ã— accumulation_steps**

- `batch_size=8, accumulation_steps=4` â†’ æ•ˆæœç­‰åŒ `batch_size=32`
- æ˜¾å­˜å ç”¨ï¼šåªæŒ‰ `batch_size=8` è®¡ç®—
- è®­ç»ƒæ•ˆæœï¼šå’Œ `batch_size=32` å‡ ä¹ä¸€æ ·ï¼Œåªæ˜¯ç¨æ…¢

## âš¡ æœåŠ¡å™¨è®­ç»ƒå®Œæ•´ç¤ºä¾‹

### Linux/Mac (bash)
```bash
#!/bin/bash
conda activate flower
export HF_ENDPOINT=https://hf-mirror.com

# æ ¹æ®æ˜¾å¡é€‰æ‹©
python train.py --config-name train_medium_gpu

# åå°è¿è¡Œ
nohup python train.py --config-name train_medium_gpu > train.log 2>&1 &
tail -f train.log
```

### Windows (PowerShell)
```powershell
conda activate flower
$env:HF_ENDPOINT="https://hf-mirror.com"

# ç›´æ¥è¿è¡Œ
python train.py --config-name train_medium_gpu

# ç›‘æ§æ—¥å¿—ï¼ˆå¦å¼€ç»ˆç«¯ï¼‰
tensorboard --logdir results/logs
```

## ğŸ› å¸¸è§é—®é¢˜

### Q: è¿˜æ˜¯æ˜¾å­˜ä¸å¤Ÿï¼Ÿ
```bash
# æ–¹æ¡ˆ1: å‡å° batch_size
python train.py --config-name train_small_gpu dataset.batch_size=4

# æ–¹æ¡ˆ2: å¢åŠ ç´¯ç§¯æ­¥æ•°
python train.py --config-name train_small_gpu \
  dataset.batch_size=4 \
  training.accumulation_steps=8  # æœ‰æ•ˆbatch=32
```

### Q: æƒ³ç”¨ä¸åŒçš„æ•°æ®å¢å¼ºï¼Ÿ
```bash
# ä½¿ç”¨è½»é‡å¢å¼ºï¼ˆæ›´å¿«ï¼‰
python train.py --config-name train_medium_gpu augmentation=light

# ä½¿ç”¨ä¸­ç­‰å¢å¼º
python train.py --config-name train_medium_gpu augmentation=medium

# ä½¿ç”¨è¶…å¼ºå¢å¼ºï¼ˆå½“å‰é»˜è®¤ï¼‰
python train.py --config-name train_medium_gpu augmentation=ultra_strong
```

### Q: æƒ³æ”¹å­¦ä¹ ç‡ï¼Ÿ
```bash
python train.py --config-name train_medium_gpu training.optimizer.lr=0.00005
```

### Q: æƒ³ä» checkpoint æ¢å¤ï¼Ÿ
ç¼–è¾‘ `train.py` ç¬¬ 60 è¡Œé™„è¿‘ï¼Œå–æ¶ˆæ³¨é‡Šï¼š
```python
# åŠ è½½ checkpoint æ¢å¤è®­ç»ƒ
checkpoint = torch.load('results/checkpoints/checkpoint_epoch_50.pt')
self.model.load_state_dict(checkpoint['model_state_dict'])
self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
self.current_epoch = checkpoint['epoch']
```

## ğŸ“ é…ç½®æ–‡ä»¶ä½ç½®

```
configs/
â”œâ”€â”€ train_small_gpu.yaml      # å°æ˜¾å¡é…ç½®
â”œâ”€â”€ train_medium_gpu.yaml     # ä¸­ç­‰æ˜¾å¡é…ç½®  
â”œâ”€â”€ train_large_gpu.yaml      # å¤§æ˜¾å¡é…ç½®
â”œâ”€â”€ train_quick_test.yaml     # å¿«é€Ÿæµ‹è¯•é…ç½®
â””â”€â”€ high_performance.yaml     # ä¹‹å‰çš„é«˜æ€§èƒ½é…ç½®ï¼ˆå·²åºŸå¼ƒï¼Œç”¨ä¸Šé¢çš„ï¼‰
```

---

**æ¨èç”¨æ³•ï¼šå…ˆç”¨ `train_quick_test` éªŒè¯æµç¨‹ï¼Œå†æ ¹æ®æ˜¾å¡ç”¨å¯¹åº”é…ç½®è®­ç»ƒï¼**
