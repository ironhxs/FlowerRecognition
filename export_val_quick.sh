#!/bin/bash
# å¿«é€Ÿå¯¼å‡ºéªŒè¯é›†ï¼ˆè‡ªåŠ¨å¤åˆ¶å›¾ç‰‡ï¼‰

echo "========================================================================"
echo "ğŸ“¦ å¯¼å‡ºéªŒè¯é›†æ•°æ®é›†"
echo "========================================================================"
echo ""

# è¿è¡Œå¯¼å‡ºè„šæœ¬ï¼ˆè‡ªåŠ¨æ¨¡å¼ï¼‰
python << 'EOF'
import os
import sys
from pathlib import Path
import pandas as pd
import shutil
from sklearn.model_selection import train_test_split
from tqdm import tqdm

# é…ç½®å‚æ•°ï¼ˆå’Œè®­ç»ƒé…ç½®ä¿æŒä¸€è‡´ï¼‰
TRAIN_CSV = "./datasets/train.csv"
TRAIN_DIR = "./datasets/train"
VAL_SPLIT = 0.15
SEED = 42
OUTPUT_DIR = "./exported_val_dataset"

print("é…ç½®å‚æ•°:")
print(f"  Val Split: {VAL_SPLIT} ({VAL_SPLIT*100:.1f}%)")
print(f"  Random Seed: {SEED}")
print()

# è¯»å–è®­ç»ƒæ•°æ®
df = pd.read_csv(TRAIN_CSV)
print(f"âœ… åŠ è½½è®­ç»ƒæ•°æ®: {len(df)} ä¸ªæ ·æœ¬")

# åˆ†å±‚åˆ’åˆ†
train_ids, val_ids, train_labels, val_labels = train_test_split(
    df['image_id'].tolist(),
    df['label'].tolist(),
    test_size=VAL_SPLIT,
    random_state=SEED,
    stratify=df['label'].tolist()
)

print(f"âœ… åˆ’åˆ†å®Œæˆ:")
print(f"   Train: {len(train_ids)} æ ·æœ¬")
print(f"   Val:   {len(val_ids)} æ ·æœ¬")
print()

# åˆ›å»ºè¾“å‡ºç›®å½•
os.makedirs(OUTPUT_DIR, exist_ok=True)
os.makedirs(f"{OUTPUT_DIR}/val_images", exist_ok=True)

# ä¿å­˜ val.csv
val_df = pd.DataFrame({
    'image_id': val_ids,
    'label': val_labels
})
val_df = val_df.sort_values('image_id').reset_index(drop=True)
val_df.to_csv(f"{OUTPUT_DIR}/val.csv", index=False)
print(f"âœ… ä¿å­˜æ ‡ç­¾æ–‡ä»¶: {OUTPUT_DIR}/val.csv")

# å¤åˆ¶å›¾ç‰‡
print(f"ğŸ“ å¤åˆ¶éªŒè¯é›†å›¾ç‰‡...")
copied = 0
missing = 0

for image_id in tqdm(val_ids, desc="å¤åˆ¶ä¸­"):
    src = f"{TRAIN_DIR}/{image_id}"
    dst = f"{OUTPUT_DIR}/val_images/{image_id}"
    
    if os.path.exists(src):
        shutil.copy2(src, dst)
        copied += 1
    else:
        missing += 1
        print(f"âš ï¸  ç¼ºå¤±: {image_id}")

print()
print(f"âœ… å¤åˆ¶å®Œæˆ: {copied} å¼ å›¾ç‰‡")
if missing > 0:
    print(f"âš ï¸  ç¼ºå¤±: {missing} å¼ å›¾ç‰‡")
print()

# ç»Ÿè®¡ä¿¡æ¯
label_counts = val_df['label'].value_counts().sort_index()
print("ğŸ“Š éªŒè¯é›†ç»Ÿè®¡:")
print(f"  æ€»æ ·æœ¬æ•°: {len(val_df)}")
print(f"  ç±»åˆ«æ•°: {val_df['label'].nunique()}")
print(f"  æœ€å°ç±»åˆ«æ ·æœ¬æ•°: {label_counts.min()}")
print(f"  æœ€å¤§ç±»åˆ«æ ·æœ¬æ•°: {label_counts.max()}")
print(f"  å¹³å‡æ¯ç±»æ ·æœ¬æ•°: {label_counts.mean():.1f}")
print()

# ç”Ÿæˆ README
with open(f"{OUTPUT_DIR}/README.txt", 'w') as f:
    f.write("éªŒè¯é›†å¯¼å‡ºä¿¡æ¯\n")
    f.write("=" * 70 + "\n\n")
    f.write(f"åˆ’åˆ†å‚æ•°:\n")
    f.write(f"  Val Split: {VAL_SPLIT} ({VAL_SPLIT*100:.1f}%)\n")
    f.write(f"  Random Seed: {SEED}\n")
    f.write(f"  åˆ†å±‚é‡‡æ ·: æ˜¯\n\n")
    f.write(f"æ•°æ®ç»Ÿè®¡:\n")
    f.write(f"  æ€»æ ·æœ¬æ•°: {len(val_df)}\n")
    f.write(f"  ç±»åˆ«æ•°: {val_df['label'].nunique()}\n\n")
    f.write("æ–‡ä»¶ç»“æ„:\n")
    f.write("  val.csv - æ ‡ç­¾æ–‡ä»¶ (image_id, label)\n")
    f.write("  val_images/ - å›¾ç‰‡ç›®å½•\n")

print("âœ… ç”Ÿæˆå…ƒæ•°æ®: README.txt")
print()

print("=" * 70)
print("ğŸ‰ å¯¼å‡ºå®Œæˆï¼")
print("=" * 70)

EOF

# æ‰“åŒ…
echo ""
echo "ğŸ“¦ æ‰“åŒ…éªŒè¯é›†..."
cd exported_val_dataset
tar -czf ../val_dataset.tar.gz ./*
cd ..

echo "âœ… æ‰“åŒ…å®Œæˆ: val_dataset.tar.gz"
echo ""
echo "ğŸ“Š æ–‡ä»¶å¤§å°:"
du -h val_dataset.tar.gz
echo ""

echo "========================================================================"
echo "ğŸš€ ä½¿ç”¨æ–¹æ³•"
echo "========================================================================"
echo ""
echo "1ï¸âƒ£  ä¼ è¾“åˆ°ç›®æ ‡æœåŠ¡å™¨:"
echo "   scp val_dataset.tar.gz user@server:/path/to/destination/"
echo ""
echo "2ï¸âƒ£  åœ¨ç›®æ ‡æœåŠ¡å™¨è§£å‹:"
echo "   tar -xzf val_dataset.tar.gz"
echo ""
echo "3ï¸âƒ£  éªŒè¯æ•°æ®å®Œæ•´æ€§:"
echo "   python -c \"import pandas as pd; df=pd.read_csv('val.csv'); print(f'æ ·æœ¬æ•°: {len(df)}')\""
echo ""
echo "========================================================================"
