# é…ç½®æ–‡ä»¶å¯¹æ¯”è¡¨

## ğŸ“Š å®Œæ•´å¯¹æ¯”

| é…ç½®æ–‡ä»¶ | ç›®æ ‡æ˜¾å¡ | æ˜¾å­˜éœ€æ±‚ | æ¨¡å‹ | æ¨¡å‹å¤§å° | Batch Size | æ¢¯åº¦ç´¯ç§¯ | æœ‰æ•ˆBatch | è®­ç»ƒæ—¶é•¿(100è½®) | é¢„æœŸå‡†ç¡®ç‡ |
|---------|---------|---------|------|---------|-----------|---------|----------|--------------|-----------|
| `train_small_gpu.yaml` | RTX 3060/3070 | 8-12GB | ConvNeXt Base | 340MB | 8 | 4 | 32 | 8-12å°æ—¶ | 87-90% |
| `train_medium_gpu.yaml` | RTX 3080/3090/4070 | 16-20GB | EfficientNetV2-L | 450MB | 16 | 2 | 32 | 5-8å°æ—¶ | 88-92% |
| `train_large_gpu.yaml` | RTX 4090/A100 | 24-32GB | EfficientNetV2-L | 450MB | 32 | 1 | 32 | 3-5å°æ—¶ | 88-92% |
| `train_rtx5090.yaml` | **RTX 5090** | 32GB | EfficientNetV2-L | 450MB | **64** | 1 | **64** | **2-3å°æ—¶** | **90-93%** |
| `train_quick_test.yaml` | ä»»æ„ | 8GB+ | ConvNeXt Tiny | 110MB | 16 | 1 | 16 | 20åˆ†é’Ÿ(5è½®) | 80-85% |
| `train_rtx5090_ensemble.yaml` | RTX 5090 | 32GB | ConvNeXt Large | 490MB | 48 | 1 | 48 | 4-6å°æ—¶ | é›†æˆå94%+ |

## ğŸ¯ å…³é”®å·®å¼‚

### 1ï¸âƒ£ æ¨¡å‹é€‰æ‹©
- **Small/Medium/Large/5090**: EfficientNetV2-L æˆ– ConvNeXtï¼ˆæœ€å¼ºæ¨¡å‹ï¼‰
- **Quick Test**: ConvNeXt Tinyï¼ˆæœ€å°æ¨¡å‹ï¼Œåªç”¨äºæµ‹è¯•ï¼‰
- **5090 Ensemble**: ConvNeXt Largeï¼ˆç”¨äºé›†æˆå­¦ä¹ ï¼‰

### 2ï¸âƒ£ Batch Size ç­–ç•¥
```
æœ‰æ•ˆBatch = batch_size Ã— accumulation_steps

Small GPU:    8 Ã— 4 = 32  ï¼ˆæ˜¾å­˜ä¸å¤Ÿï¼Œç”¨æ¢¯åº¦ç´¯ç§¯å‡‘ï¼‰
Medium GPU:  16 Ã— 2 = 32  ï¼ˆåˆšå¥½å¤Ÿç”¨ï¼‰
Large GPU:   32 Ã— 1 = 32  ï¼ˆç›´æ¥ç”¨å¤§batchï¼‰
RTX 5090:    64 Ã— 1 = 64  ï¼ˆæ›´å¤§batchï¼Œæ›´å¿«æ”¶æ•›ï¼‰
```

### 3ï¸âƒ£ å­¦ä¹ ç‡è°ƒæ•´
- **Small/Medium/Large**: `lr=0.0001`ï¼ˆæ ‡å‡†ï¼‰
- **RTX 5090**: `lr=0.0002`ï¼ˆå¤§batchéœ€è¦æ›´é«˜å­¦ä¹ ç‡ï¼‰
- **è§„åˆ™**: `lr_new = lr_base Ã— sqrt(batch_new / batch_base)`

### 4ï¸âƒ£ è®­ç»ƒé€Ÿåº¦
```
å‡è®¾æ•°æ®é›† 19,928 å¼ ï¼Œ100 epochsï¼š

Small GPU (batch=8):   
  - æ¯è½®: 2,491 æ­¥
  - å•è½®æ—¶é•¿: ~5åˆ†é’Ÿ
  - æ€»æ—¶é•¿: 8-12å°æ—¶

Medium GPU (batch=16): 
  - æ¯è½®: 1,246 æ­¥
  - å•è½®æ—¶é•¿: ~3åˆ†é’Ÿ
  - æ€»æ—¶é•¿: 5-8å°æ—¶

Large GPU (batch=32):  
  - æ¯è½®: 623 æ­¥
  - å•è½®æ—¶é•¿: ~2åˆ†é’Ÿ
  - æ€»æ—¶é•¿: 3-5å°æ—¶

RTX 5090 (batch=64):   
  - æ¯è½®: 312 æ­¥
  - å•è½®æ—¶é•¿: ~1.5åˆ†é’Ÿ
  - æ€»æ—¶é•¿: 2-3å°æ—¶ âš¡
```

## ğŸš€ ä½¿ç”¨å»ºè®®

### ä½ çš„ RTX 5090 ä¸“ç”¨æ–¹æ¡ˆ

#### æ–¹æ¡ˆA: å•æ¨¡å‹æœ€å¼ºï¼ˆæ¨èæ–°æ‰‹ï¼‰
```bash
python train.py --config-name train_rtx5090
```
- æœ€ç®€å•ï¼Œä¸€æ¡å‘½ä»¤æå®š
- EfficientNetV2-L + Batch 64
- é¢„æœŸ: 90-93% å‡†ç¡®ç‡
- è®­ç»ƒæ—¶é—´: 2-3å°æ—¶

#### æ–¹æ¡ˆB: æ¨¡å‹é›†æˆï¼ˆæ¨èç«èµ›ï¼‰
```bash
# åŒæ—¶å¼€3ä¸ªç»ˆç«¯ï¼Œè®­ç»ƒ3ä¸ªä¸åŒæ¨¡å‹
# ç»ˆç«¯1
python train.py --config-name train_rtx5090_ensemble model=efficientnet_v2_l_optimized

# ç»ˆç«¯2  
python train.py --config-name train_rtx5090_ensemble model=convnext_large

# ç»ˆç«¯3
python train.py --config-name train_rtx5090_ensemble model=swin_transformer_v2
```
- 3ä¸ªæ¨¡å‹å¹¶è¡Œè®­ç»ƒï¼ˆ5090æ˜¾å­˜å¤Ÿç”¨ï¼‰
- æœ€åé›†æˆé¢„æµ‹ï¼ˆæŠ•ç¥¨æˆ–å¹³å‡ï¼‰
- é¢„æœŸ: 94-96% å‡†ç¡®ç‡ ğŸ†
- è®­ç»ƒæ—¶é—´: 4-6å°æ—¶ï¼ˆ3ä¸ªå¹¶è¡Œï¼‰

#### æ–¹æ¡ˆC: æé™ Batchï¼ˆå®éªŒæ€§ï¼‰
```bash
python train.py --config-name train_rtx5090 \
  dataset.batch_size=96 \
  training.optimizer.lr=0.00025
```
- batch_size=96ï¼ˆéœ€è¦ç›‘æ§æ˜¾å­˜ï¼‰
- å¦‚æœ OOMï¼Œé™åˆ° 80 æˆ– 64

## ğŸ“‹ é…ç½®æ–‡ä»¶è¯¦ç»†è¯´æ˜

### `train_small_gpu.yaml`
```yaml
model: convnext_base        # æ›´å°çš„æ¨¡å‹
batch_size: 8               # å°batché€‚åº”å°æ˜¾å­˜
accumulation_steps: 4       # é€šè¿‡ç´¯ç§¯æ¨¡æ‹Ÿå¤§batch
num_workers: 4              # è¾ƒå°‘çš„æ•°æ®åŠ è½½çº¿ç¨‹
```
**é€‚ç”¨**: é¢„ç®—å¡ã€ç¬”è®°æœ¬GPU

### `train_medium_gpu.yaml`
```yaml
model: efficientnet_v2_l_optimized  # æœ€å¼ºæ¨¡å‹
batch_size: 16                       # ä¸­ç­‰batch
accumulation_steps: 2                # å°‘é‡ç´¯ç§¯
num_workers: 8                       # æ›´å¤šçº¿ç¨‹
```
**é€‚ç”¨**: ä¸»æµæ¸¸æˆå¡

### `train_large_gpu.yaml`
```yaml
model: efficientnet_v2_l_optimized
batch_size: 32              # å¤§batchï¼Œä¸éœ€è¦ç´¯ç§¯
accumulation_steps: 1       
num_workers: 16             # å……åˆ†åˆ©ç”¨CPU
```
**é€‚ç”¨**: é«˜ç«¯å¡ã€ä¸“ä¸šå¡

### `train_rtx5090.yaml` â­
```yaml
model: efficientnet_v2_l_optimized
batch_size: 64              # è¶…å¤§batchï¼
lr: 0.0002                  # æ›´é«˜å­¦ä¹ ç‡åŒ¹é…å¤§batch
num_workers: 16             
```
**é€‚ç”¨**: RTX 5090ï¼ˆä½ çš„å¡ï¼‰

### `train_quick_test.yaml`
```yaml
model: convnext_tiny        # æœ€å°æ¨¡å‹
batch_size: 16
epochs: 5                   # åªè·‘5è½®æµ‹è¯•
augmentation: medium        # è½»é‡å¢å¼º
```
**é€‚ç”¨**: å¿«é€ŸéªŒè¯æµç¨‹ï¼ˆæ‰€æœ‰å¡éƒ½èƒ½è·‘ï¼‰

### `train_rtx5090_ensemble.yaml`
```yaml
model: convnext_large       # å¯æ›¿æ¢ä¸ºå…¶ä»–æ¨¡å‹
batch_size: 48              # ç»™å¤šæ¨¡å‹å¹¶è¡Œç•™ç©ºé—´
```
**é€‚ç”¨**: RTX 5090 é›†æˆå­¦ä¹ 

## ğŸ“ ä¸ºä»€ä¹ˆå¤§ batch æ›´å¥½ï¼Ÿ

### ä¼˜ç‚¹
1. **è®­ç»ƒæ›´å¿«**: æ­¥æ•°å‡å°‘ä¸€åŠ
2. **æ›´ç¨³å®š**: æ¢¯åº¦ä¼°è®¡æ›´å‡†ç¡®
3. **æ›´é«˜å‡†ç¡®ç‡**: batch=64 é€šå¸¸æ¯” batch=32 é«˜ 1-2%

### ç¼ºç‚¹
1. **éœ€è¦å¤§æ˜¾å­˜**: æ¯å¼ å›¾å ç”¨æ˜¾å­˜
2. **éœ€è¦è°ƒå­¦ä¹ ç‡**: å¤§batchè¦é…é«˜å­¦ä¹ ç‡
3. **å¯èƒ½æ¬ æ‹Ÿåˆ**: å¤ªå¤§çš„batchä¼šå‡å°‘éšæœºæ€§

### 5090 çš„ä¼˜åŠ¿
- 32GB æ˜¾å­˜ â†’ å¯ä»¥ç”¨ batch=64-96
- æ›´å¿«çš„è®¡ç®— â†’ è®­ç»ƒæ—¶é—´å‡åŠ
- æ›´é«˜å¸¦å®½ â†’ æ•°æ®åŠ è½½ä¸æ˜¯ç“¶é¢ˆ

## ğŸ’¡ å¿«é€Ÿå†³ç­–æ ‘

```
ä½ æœ‰ 5090ï¼Ÿ
  â”œâ”€ æ˜¯ï¼Œç¬¬ä¸€æ¬¡ç”¨ â†’ train_quick_test (5åˆ†é’ŸéªŒè¯)
  â”‚   â””â”€ éªŒè¯é€šè¿‡ â†’ train_rtx5090 (2-3å°æ—¶å®Œæ•´è®­ç»ƒ)
  â”‚
  â””â”€ æ˜¯ï¼Œè¦æ‰“æ¯”èµ› â†’ train_rtx5090_ensemble (é›†æˆå­¦ä¹ )
      â””â”€ åŒæ—¶è·‘3ä¸ªæ¨¡å‹ â†’ å‡†ç¡®ç‡ +2-4%

ä½ æ˜¯å…¶ä»–å¡ï¼Ÿ
  â”œâ”€ 8-12GB (3060/3070) â†’ train_small_gpu
  â”œâ”€ 16-20GB (3080/4070) â†’ train_medium_gpu
  â””â”€ 24-32GB (4090/A100) â†’ train_large_gpu
```

## ğŸ”¥ RTX 5090 æœ€ä½³å®è·µ

1. **å…ˆæµ‹è¯•**
   ```bash
   python train.py --config-name train_quick_test
   ```

2. **å•æ¨¡å‹è®­ç»ƒ**
   ```bash
   python train.py --config-name train_rtx5090
   ```

3. **é›†æˆå­¦ä¹ ï¼ˆç«èµ›æ¨èï¼‰**
   ```bash
   # å¼€3ä¸ªç»ˆç«¯
   python train.py --config-name train_rtx5090_ensemble model=efficientnet_v2_l_optimized
   python train.py --config-name train_rtx5090_ensemble model=convnext_large
   python train.py --config-name train_rtx5090_ensemble model=swin_transformer_v2
   ```

4. **æé™è°ƒä¼˜**
   ```bash
   python train.py --config-name train_rtx5090 dataset.batch_size=96
   ```

---

**æ€»ç»“**: ä½ æœ‰ 5090 å°±ç›´æ¥ç”¨ `train_rtx5090.yaml`ï¼Œæƒ³è¦æ›´é«˜åˆ†å°±ç”¨ `train_rtx5090_ensemble.yaml` è®­ç»ƒ3ä¸ªæ¨¡å‹é›†æˆï¼
