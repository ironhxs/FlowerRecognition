# Swin Transformer V2 Training Config
# Transformer-based model - 350MB, excellent for fine-grained recognition

defaults:
  - model: swin_transformer_v2
  - dataset: flower100
  - training: focal
  - augmentation: ultra_strong
  - _self_

# Dataset settings
dataset:
  batch_size: 16
  num_workers: 8
  
# Training settings
training:
  epochs: 100
  accumulation_steps: 2  # Simulate batch_size=32
  
# Project Settings
project_name: flower_recognition
experiment_name: swin_transformer_v2
seed: 42

# Paths
data_dir: ./data
output_dir: ./results
checkpoint_dir: ${output_dir}/checkpoints
log_dir: ${output_dir}/logs

# Hardware
device: cuda
num_workers: 8
pin_memory: true

# Performance:
# - Model size: ~350MB
# - Training time: ~6-9 hours (100 epochs)
# - Expected accuracy: 87-90%
# - VRAM usage: ~12-14GB
# - Use case: Ensemble diversity, attention-based features
# - Note: Swin is great for capturing spatial relationships in flowers
