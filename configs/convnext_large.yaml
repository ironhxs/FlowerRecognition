# ConvNeXt Large Training Config
# Large model - 490MB, near competition size limit

defaults:
  - model: convnext_large
  - dataset: flower100
  - training: focal
  - augmentation: ultra_strong
  - _self_

# Dataset settings
dataset:
  batch_size: 12  # Larger model needs smaller batch
  num_workers: 8
  
# Training settings
training:
  epochs: 100
  accumulation_steps: 1  # Full batch_size=32
  val_every: 1  # Validate every N epochs
  save_every: 5  # Save checkpoint every N epochs
  early_stopping:
    patience: 10
    min_delta: 0.001
  
# Project Settings
project_name: flower_recognition
experiment_name: convnext_large
seed: 42

# Paths
data_dir: ./datasets
output_dir: ./results
checkpoint_dir: ${output_dir}/checkpoints
log_dir: ${output_dir}/logs

# Hardware
device: cuda
num_workers: 8
pin_memory: true

# Performance:
# - Model size: ~490MB (close to 500MB limit!)
# - Training time: ~6-10 hours (100 epochs)
# - Expected accuracy: 88-91%
# - VRAM usage: ~16-20GB
# - Use case: Maximum model capacity, ensemble member
