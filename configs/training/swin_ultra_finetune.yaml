# Ultra Fine-tune Configuration
# Very low LR to avoid overfitting

optimizer:
  name: AdamW
  lr: 2.0e-6  # 超低学习率 (原来1e-5)
  betas: [0.9, 0.999]
  weight_decay: 0.1  # 增大weight decay
  eps: 1.0e-8

scheduler:
  name: cosine
  warmup_epochs: 2
  min_lr: 1.0e-7
  warmup_lr: 1.0e-6

epochs: 50
batch_size: 20
accumulation_steps: 1
val_every: 1
save_every: 5

# 更强的正则化
label_smoothing: 0.15  # 增大 (原来0.1)
drop_path_rate: 0.3  # 增大 (原来0.2)

loss_function: focal
focal_alpha: 0.25
focal_gamma: 2.0

early_stopping:
  patience: 20  # 减小patience
  min_delta: 0.001  # 提高阈值
  mode: max
  monitor: val_acc

use_amp: true
clip_grad_norm: 1.0
