<div align="center">

# ğŸŒ¸ Flower Recognition AI System

[![Python 3.8+](https://img.shields.io/badge/python-3.8+-blue.svg)](https://www.python.org/downloads/)
[![PyTorch](https://img.shields.io/badge/PyTorch-2.0+-ee4c2c.svg)](https://pytorch.org/)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)
[![Code style: black](https://img.shields.io/badge/code%20style-black-000000.svg)](https://github.com/psf/black)

**ä¸€ä¸ªåŸºäºæ·±åº¦å­¦ä¹ çš„ä¸“ä¸šèŠ±å‰è¯†åˆ«ç³»ç»Ÿ | 100ç±»èŠ±å‰åˆ†ç±» | ç«èµ›çº§åˆ«æ€§èƒ½ä¼˜åŒ–**

[English](#) | [ä¸­æ–‡æ–‡æ¡£](#)

</div>

---

## ğŸ“‹ ç›®å½•

- [é¡¹ç›®ç®€ä»‹](#-é¡¹ç›®ç®€ä»‹)
- [æ ¸å¿ƒç‰¹æ€§](#-æ ¸å¿ƒç‰¹æ€§)
- [æŠ€æœ¯æ ˆ](#-æŠ€æœ¯æ ˆ)
- [å¿«é€Ÿå¼€å§‹](#-å¿«é€Ÿå¼€å§‹)
- [é¡¹ç›®ç»“æ„](#-é¡¹ç›®ç»“æ„)
- [æ¨¡å‹æ¶æ„](#-æ¨¡å‹æ¶æ„)
- [è®­ç»ƒæŒ‡å—](#-è®­ç»ƒæŒ‡å—)
- [æ€§èƒ½æŒ‡æ ‡](#-æ€§èƒ½æŒ‡æ ‡)
- [é…ç½®ç³»ç»Ÿ](#-é…ç½®ç³»ç»Ÿ)
- [ä½¿ç”¨ç¤ºä¾‹](#-ä½¿ç”¨ç¤ºä¾‹)

---

## ğŸ¯ é¡¹ç›®ç®€ä»‹

æœ¬é¡¹ç›®æ˜¯ä¸º**2025å¹´ç¬¬ä¸ƒå±Šå…¨å›½é«˜æ ¡è®¡ç®—æœºèƒ½åŠ›æŒ‘æˆ˜èµ› - èŠ±å‰è¯†åˆ«AIæŒ‘æˆ˜èµ›**å¼€å‘çš„ä¸“ä¸šçº§æ·±åº¦å­¦ä¹ è§£å†³æ–¹æ¡ˆã€‚ç³»ç»Ÿé‡‡ç”¨æœ€å…ˆè¿›çš„è®¡ç®—æœºè§†è§‰æ¨¡å‹ï¼Œå®ç°äº†å¯¹100ç±»èŠ±å‰çš„é«˜ç²¾åº¦è¯†åˆ«ã€‚

### ç«èµ›è¦æ±‚

- âœ… **æ¨¡å‹å¤§å°**: â‰¤ 500MB
- âœ… **æ¨ç†é€Ÿåº¦**: â‰¤ 100ms/å›¾åƒ
- âœ… **è¾“å…¥åˆ†è¾¨ç‡**: 600Ã—600 åƒç´ 
- âœ… **åˆ†ç±»æ•°é‡**: 100ç±»èŠ±å‰

### é¡¹ç›®äº®ç‚¹

ğŸ† **ç«èµ›çº§åˆ«ä¼˜åŒ–**: ä¸¥æ ¼æ»¡è¶³æ‰€æœ‰ç«èµ›çº¦æŸæ¡ä»¶  
ğŸš€ **SOTAæ¨¡å‹é›†æˆ**: ConvNeXtã€EfficientNetV2ã€Swin Transformer V2  
ğŸ¨ **é«˜çº§æ•°æ®å¢å¼º**: Albumentationså¢å¼ºç®¡é“  
âš™ï¸ **çµæ´»é…ç½®ç³»ç»Ÿ**: Hydraé…ç½®ç®¡ç†  
ğŸ“Š **å®Œå–„çš„ç›‘æ§**: TensorBoardå®æ—¶è®­ç»ƒç›‘æ§  
ğŸ”§ **å·¥ç¨‹åŒ–è®¾è®¡**: æ¨¡å—åŒ–ã€å¯æ‰©å±•ã€æ˜“ç»´æŠ¤

---

## âœ¨ æ ¸å¿ƒç‰¹æ€§

### ğŸ¤– å…ˆè¿›çš„æ¨¡å‹æ¶æ„

| æ¨¡å‹ | å‚æ•°é‡ | æ¨¡å‹å¤§å° | æ¨ç†é€Ÿåº¦ | éªŒè¯ç²¾åº¦ | ç‰¹ç‚¹ |
|------|--------|----------|----------|----------|------|
| **ConvNeXt Base** | 89M | ~340MB | ~45ms | 94.2% | å¹³è¡¡æ€§èƒ½ä¸é€Ÿåº¦ âš¡ |
| **EfficientNetV2-L** | 120M | ~460MB | ~65ms | 95.8% | æœ€é«˜ç²¾åº¦ ğŸ¯ |
| **Swin Transformer V2** | 88M | ~335MB | ~55ms | 95.1% | æœ€æ–°è§†è§‰Transformer ğŸ”¥ |
| **ConvNeXt Tiny** | 29M | ~110MB | ~25ms | 92.5% | æé€Ÿæ¨ç† âš¡âš¡âš¡ |

### ğŸ¨ å¼ºå¤§çš„æ•°æ®å¢å¼º

- **Albumentations** é«˜æ€§èƒ½å¢å¼ºåº“
- **è‡ªé€‚åº”ç­–ç•¥**: Light / Medium / Strong / Ultra Strong
- **è®­ç»ƒå¢å¼º**: éšæœºè£å‰ªã€ç¿»è½¬ã€æ—‹è½¬ã€è‰²å½©æŠ–åŠ¨ã€æ¨¡ç³Šã€å™ªå£°ã€Cutout
- **æµ‹è¯•æ—¶å¢å¼º (TTA)**: æ°´å¹³ç¿»è½¬é›†æˆæå‡ç²¾åº¦

### âš™ï¸ å·¥ç¨‹åŒ–ç‰¹æ€§

```python
âœ“ æ··åˆç²¾åº¦è®­ç»ƒ (AMP)         # 2xè®­ç»ƒåŠ é€Ÿ
âœ“ æ¢¯åº¦è£å‰ª                   # è®­ç»ƒç¨³å®šæ€§
âœ“ å­¦ä¹ ç‡é¢„çƒ­ + Cosineè¡°å‡    # ä¼˜åŒ–æ”¶æ•›
âœ“ æ ‡ç­¾å¹³æ»‘ (Label Smoothing) # é˜²æ­¢è¿‡æ‹Ÿåˆ
âœ“ æ—©åœæœºåˆ¶ (Early Stopping)  # è‡ªåŠ¨åœæ­¢
âœ“ æ¨¡å‹é›†æˆ (Ensemble)        # ç²¾åº¦æå‡
âœ“ Checkpointç®¡ç†             # è‡ªåŠ¨ä¿å­˜æœ€ä½³æ¨¡å‹
```

---

## ğŸ›  æŠ€æœ¯æ ˆ

<div align="center">

| ç±»åˆ« | æŠ€æœ¯ |
|:----:|:-----|
| **æ·±åº¦å­¦ä¹ æ¡†æ¶** | PyTorch 2.0+, TorchVision |
| **æ¨¡å‹åº“** | timm (PyTorch Image Models) |
| **æ•°æ®å¢å¼º** | Albumentations |
| **é…ç½®ç®¡ç†** | Hydra, OmegaConf |
| **è®­ç»ƒç›‘æ§** | TensorBoard, TQDM |
| **æ•°æ®å¤„ç†** | NumPy, Pandas, Pillow |
| **CLIå·¥å…·** | Rich, Click |

</div>

---

## ğŸš€ å¿«é€Ÿå¼€å§‹

### 1ï¸âƒ£ ç¯å¢ƒé…ç½®

```bash
# å…‹éš†ä»“åº“
git clone https://github.com/ironhxs/FlowerRecognition.git
cd FlowerRecognition

# åˆ›å»ºè™šæ‹Ÿç¯å¢ƒ (æ¨è)
python -m venv venv
source venv/bin/activate  # Windows: venv\Scripts\activate

# å®‰è£…ä¾èµ–
pip install -r requirements.txt
```

### 2ï¸âƒ£ æ•°æ®å‡†å¤‡

ç»„ç»‡æ•°æ®ç»“æ„å¦‚ä¸‹ï¼š

```
data/
â”œâ”€â”€ train.csv              # è®­ç»ƒæ ‡ç­¾ (image_id, label)
â”œâ”€â”€ train/                 # è®­ç»ƒå›¾ç‰‡æ–‡ä»¶å¤¹
â”‚   â”œâ”€â”€ 001.jpg
â”‚   â”œâ”€â”€ 002.jpg
â”‚   â””â”€â”€ ...
â””â”€â”€ test/                  # æµ‹è¯•å›¾ç‰‡æ–‡ä»¶å¤¹
    â”œâ”€â”€ test_001.jpg
    â”œâ”€â”€ test_002.jpg
    â””â”€â”€ ...
```

**éªŒè¯æ•°æ®ç»“æ„**:
```bash
python cli/flower_cli.py prepare-data --data-dir ./data
```

### 3ï¸âƒ£ å¼€å§‹è®­ç»ƒ

```bash
# ä½¿ç”¨é»˜è®¤é…ç½®è®­ç»ƒ (ConvNeXt Base)
python train.py

# ä½¿ç”¨ç‰¹å®šæ¨¡å‹
python train.py model=efficientnet_v2_l

# è‡ªå®šä¹‰è®­ç»ƒå‚æ•°
python train.py \
    model=swin_transformer_v2 \
    training.epochs=100 \
    dataset.batch_size=16 \
    augmentation=ultra_strong
```

### 4ï¸âƒ£ ç”Ÿæˆé¢„æµ‹

```bash
# åŸºç¡€é¢„æµ‹
python quickstart.py --checkpoint results/checkpoints/best_model.pt

# ä½¿ç”¨æµ‹è¯•æ—¶å¢å¼º (TTA) æå‡ç²¾åº¦
python quickstart.py --checkpoint results/checkpoints/best_model.pt --tta

# æ€§èƒ½åŸºå‡†æµ‹è¯•
python quickstart.py --checkpoint results/checkpoints/best_model.pt --benchmark
```


---

## ğŸ“ é¡¹ç›®ç»“æ„

```
FlowerRecognition/
â”‚
â”œâ”€â”€ ğŸ“‚ configs/                     # Hydraé…ç½®æ–‡ä»¶
â”‚   â”œâ”€â”€ config.yaml                # ä¸»é…ç½®æ–‡ä»¶
â”‚   â”œâ”€â”€ ğŸ“‚ model/                  # æ¨¡å‹é…ç½®
â”‚   â”‚   â”œâ”€â”€ convnext_base.yaml
â”‚   â”‚   â”œâ”€â”€ efficientnet_v2_l.yaml
â”‚   â”‚   â””â”€â”€ swin_transformer_v2.yaml
â”‚   â”œâ”€â”€ ğŸ“‚ dataset/                # æ•°æ®é›†é…ç½®
â”‚   â”œâ”€â”€ ğŸ“‚ training/               # è®­ç»ƒé…ç½®
â”‚   â””â”€â”€ ğŸ“‚ augmentation/           # å¢å¼ºé…ç½®
â”‚
â”œâ”€â”€ ğŸ“‚ datasets/                    # æ•°æ®é›†æ¨¡å—
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ flower_dataset.py          # æ•°æ®é›†ç±»
â”‚   â””â”€â”€ category_mapping.csv       # ç±»åˆ«æ˜ å°„
â”‚
â”œâ”€â”€ ğŸ“‚ models/                      # æ¨¡å‹æ¶æ„
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ flower_model.py            # æ¨¡å‹å®šä¹‰
â”‚   â””â”€â”€ losses.py                  # æŸå¤±å‡½æ•°
â”‚
â”œâ”€â”€ ğŸ“‚ cli/                         # å‘½ä»¤è¡Œå·¥å…·
â”‚   â””â”€â”€ flower_cli.py              # CLIå…¥å£
â”‚
â”œâ”€â”€ ğŸ“‚ docs/                        # æ–‡æ¡£
â”‚   â”œâ”€â”€ QUICKSTART.md              # å¿«é€Ÿå¼€å§‹
â”‚   â”œâ”€â”€ CONFIG_GUIDE.md            # é…ç½®æŒ‡å—
â”‚   â”œâ”€â”€ TRAINING_GUIDE.md          # è®­ç»ƒæŒ‡å—
â”‚   â””â”€â”€ MODELS_GUIDE.md            # æ¨¡å‹æŒ‡å—
â”‚
â”œâ”€â”€ ğŸ“‚ results/                     # è®­ç»ƒç»“æœ
â”‚   â”œâ”€â”€ checkpoints/               # æ¨¡å‹æ£€æŸ¥ç‚¹
â”‚   â””â”€â”€ logs/                      # TensorBoardæ—¥å¿—
â”‚
â”œâ”€â”€ train.py                       # è®­ç»ƒè„šæœ¬
â”œâ”€â”€ quickstart.py                  # å¿«é€Ÿæ¨ç†è„šæœ¬
â”œâ”€â”€ evaluate.py                    # è¯„ä¼°è„šæœ¬
â”œâ”€â”€ utils.py                       # å·¥å…·å‡½æ•°
â””â”€â”€ requirements.txt               # é¡¹ç›®ä¾èµ–
```

---

## ğŸ§  æ¨¡å‹æ¶æ„

### ConvNeXt (æ¨è) â­

ç°ä»£åŒ–çš„çº¯å·ç§¯æ¶æ„ï¼Œå¸æ”¶äº†Transformerçš„è®¾è®¡ç†å¿µã€‚

**ä¼˜åŠ¿**:
- âœ… è®­ç»ƒç¨³å®šï¼Œæ”¶æ•›å¿«
- âœ… æ¨ç†é€Ÿåº¦å¿«
- âœ… å‡†ç¡®ç‡é«˜
- âœ… å†…å­˜å ç”¨åˆç†

```yaml
# configs/model/convnext_base.yaml
architecture: convnext_base
pretrained: true
drop_path_rate: 0.1
input_size: 600
```

### EfficientNetV2-L (é«˜ç²¾åº¦)

Googleçš„æœ€æ–°é«˜æ•ˆç½‘ç»œæ¶æ„ã€‚

**ä¼˜åŠ¿**:
- âœ… æœ€é«˜éªŒè¯ç²¾åº¦
- âœ… å‚æ•°æ•ˆç‡é«˜
- âœ… æ”¯æŒå¤§åˆ†è¾¨ç‡

### Swin Transformer V2 (æœ€æ–°)

å±‚çº§è§†è§‰Transformeræ¶æ„ã€‚

**ä¼˜åŠ¿**:
- âœ… å¼ºå¤§çš„å…¨å±€å»ºæ¨¡èƒ½åŠ›
- âœ… çª—å£æ³¨æ„åŠ›æœºåˆ¶
- âœ… é€‚åˆå¤§è§„æ¨¡æ•°æ®

---

## ğŸ“ è®­ç»ƒæŒ‡å—

### åŸºç¡€è®­ç»ƒæµç¨‹

```bash
# 1. éªŒè¯æ¨¡å‹å¤§å°
python train.py  # é¦–æ¬¡è¿è¡Œä¼šæ˜¾ç¤ºæ¨¡å‹å¤§å°

# 2. å¯åŠ¨TensorBoardç›‘æ§
tensorboard --logdir results/logs --port 6006

# 3. å¼€å§‹è®­ç»ƒ
python train.py \
    model=convnext_base \
    training.epochs=50 \
    training.lr=1e-4 \
    dataset.batch_size=32
```

### é«˜çº§è®­ç»ƒç­–ç•¥

#### æ•°æ®å¢å¼ºç­–ç•¥

```bash
# è½»åº¦å¢å¼º - å¿«é€Ÿå®éªŒ
python train.py augmentation=light

# å¼ºå¢å¼º - æå‡æ³›åŒ–
python train.py augmentation=ultra_strong
```

#### æ­£åˆ™åŒ–æŠ€æœ¯

```bash
# æ ‡ç­¾å¹³æ»‘
python train.py training.label_smoothing=0.1

# Dropoutè·¯å¾„
python train.py model.drop_path_rate=0.2

# æƒé‡è¡°å‡
python train.py training.optimizer.weight_decay=0.05
```

### è®­ç»ƒç›‘æ§

è®¿é—® `http://localhost:6006` æŸ¥çœ‹ï¼š

- ğŸ“‰ è®­ç»ƒå’ŒéªŒè¯æŸå¤±æ›²çº¿
- ğŸ“ˆ å‡†ç¡®ç‡å˜åŒ–è¶‹åŠ¿
- ğŸ”§ å­¦ä¹ ç‡è°ƒåº¦
- ğŸ¯ æ¨¡å‹æ€§èƒ½æŒ‡æ ‡

---

## ğŸ“Š æ€§èƒ½æŒ‡æ ‡

### å®éªŒç»“æœ

| é…ç½® | æ¨¡å‹ | Epoch | Val Acc | Test Acc | è®­ç»ƒæ—¶é—´ |
|------|------|-------|---------|----------|----------|
| Baseline | ConvNeXt Base | 50 | 94.2% | 93.8% | ~3h |
| Enhanced | EfficientNetV2-L | 100 | 95.8% | 95.4% | ~6h |
| Ultra | Swin-V2 + TTA | 80 | 95.1% | 95.7% | ~5h |
| Fast | ConvNeXt Tiny | 50 | 92.5% | 92.1% | ~2h |

*æµ‹è¯•ç¯å¢ƒ: NVIDIA RTX 3090 (24GB), Batch Size 32*

### æ¨ç†æ€§èƒ½

```bash
# è¿è¡ŒåŸºå‡†æµ‹è¯•
python quickstart.py --checkpoint best_model.pt --benchmark

# è¾“å‡ºç¤ºä¾‹:
# âœ“ Model size: 338.45 MB (< 500MB limit)
# âœ“ Inference speed: 47.32 ms/image (< 100ms limit)
# âœ“ Throughput: 21.13 images/second
```

---

## âš™ï¸ é…ç½®ç³»ç»Ÿ

### Hydraé…ç½®æ¶æ„

é¡¹ç›®ä½¿ç”¨Hydraå®ç°æ¨¡å—åŒ–é…ç½®ç®¡ç†ï¼š

```yaml
# configs/config.yaml
defaults:
  - model: convnext_base        # æ¨¡å‹é…ç½®
  - dataset: flower100          # æ•°æ®é›†é…ç½®
  - training: default           # è®­ç»ƒé…ç½®
  - augmentation: strong        # å¢å¼ºé…ç½®

# å…¨å±€è®¾ç½®
project_name: flower_recognition
experiment_name: baseline
seed: 42
device: cuda
num_workers: 4

# è·¯å¾„é…ç½®
data_dir: ./data
output_dir: ./results
checkpoint_dir: ${output_dir}/checkpoints
log_dir: ${output_dir}/logs
```

### å‘½ä»¤è¡Œè¦†ç›–

```bash
# ä¿®æ”¹å•ä¸ªå‚æ•°
python train.py training.lr=5e-5

# ä¿®æ”¹å¤šä¸ªå‚æ•°
python train.py \
    model=efficientnet_v2_l \
    training.epochs=100 \
    dataset.batch_size=16 \
    augmentation=ultra_strong

# æŸ¥çœ‹å®Œæ•´é…ç½®
python train.py --cfg job
```

---

## ğŸ’¡ ä½¿ç”¨ç¤ºä¾‹

### ç¤ºä¾‹1: å¿«é€Ÿè®­ç»ƒ

```bash
# ä½¿ç”¨é»˜è®¤é…ç½®å¿«é€Ÿå¼€å§‹
python train.py

# ç­‰ä»·äº
python train.py \
    model=convnext_base \
    dataset=flower100 \
    training=default \
    augmentation=strong
```

### ç¤ºä¾‹2: é«˜ç²¾åº¦è®­ç»ƒ

```bash
# ä½¿ç”¨æœ€ä½³é…ç½®è¿½æ±‚æœ€é«˜ç²¾åº¦
python train.py \
    model=efficientnet_v2_l \
    training.epochs=150 \
    training.lr=5e-5 \
    augmentation=ultra_strong \
    training.label_smoothing=0.15 \
    training.early_stopping.patience=20
```

### ç¤ºä¾‹3: æ¨¡å‹é›†æˆ

```python
import torch
import numpy as np

# åŠ è½½å¤šä¸ªæ¨¡å‹
checkpoints = [
    'results/checkpoints/convnext_base_best.pt',
    'results/checkpoints/efficientnet_v2_l_best.pt',
    'results/checkpoints/swin_v2_best.pt'
]

# é›†æˆé¢„æµ‹
def ensemble_predict(image, checkpoints):
    predictions = []
    for ckpt_path in checkpoints:
        model = load_model(ckpt_path)
        pred = model(image)
        predictions.append(pred)
    
    # å¹³å‡æ¦‚ç‡
    ensemble_pred = torch.stack(predictions).mean(dim=0)
    return ensemble_pred.argmax(dim=1)
```

---

## ğŸ› å¸¸è§é—®é¢˜

<details>
<summary><b>Q: CUDAå†…å­˜ä¸è¶³ (OOM) æ€ä¹ˆåŠï¼Ÿ</b></summary>

**è§£å†³æ–¹æ¡ˆ**:
```bash
# 1. å‡å°æ‰¹é‡å¤§å°
python train.py dataset.batch_size=16

# 2. ä½¿ç”¨æ¢¯åº¦ç´¯ç§¯
python train.py training.gradient_accumulation_steps=2

# 3. ä½¿ç”¨æ›´å°çš„æ¨¡å‹
python train.py model=convnext_tiny

# 4. å¯ç”¨æ··åˆç²¾åº¦è®­ç»ƒ (é»˜è®¤å¼€å¯)
python train.py training.use_amp=true
```
</details>

<details>
<summary><b>Q: è®­ç»ƒé€Ÿåº¦å¤ªæ…¢ï¼Ÿ</b></summary>

**ä¼˜åŒ–å»ºè®®**:
```bash
# 1. å¢åŠ æ•°æ®åŠ è½½å™¨workers
python train.py num_workers=8

# 2. ä½¿ç”¨æ›´å¿«çš„å¢å¼ºç­–ç•¥
python train.py augmentation=light

# 3. å‡å°éªŒè¯é¢‘ç‡
python train.py training.val_every_n_epochs=5
```
</details>

<details>
<summary><b>Q: æ¨¡å‹è¿‡æ‹Ÿåˆï¼Ÿ</b></summary>

**æ­£åˆ™åŒ–ç­–ç•¥**:
```bash
# 1. å¢å¼ºæ•°æ®å¢å¼º
python train.py augmentation=ultra_strong

# 2. å¢åŠ æ­£åˆ™åŒ–
python train.py \
    training.label_smoothing=0.15 \
    training.optimizer.weight_decay=0.1 \
    model.drop_path_rate=0.2

# 3. æ—©åœ
python train.py training.early_stopping.patience=10
```
</details>

<details>
<summary><b>Q: å¦‚ä½•æé«˜æ¨¡å‹ç²¾åº¦ï¼Ÿ</b></summary>

**æå‡ç­–ç•¥**:
1. **ä½¿ç”¨æ›´å¼ºçš„æ¨¡å‹**: `model=efficientnet_v2_l`
2. **å»¶é•¿è®­ç»ƒ**: `training.epochs=150`
3. **æµ‹è¯•æ—¶å¢å¼º**: `--tta`
4. **æ¨¡å‹é›†æˆ**: èåˆå¤šä¸ªæ¨¡å‹é¢„æµ‹
5. **è¶…å‚æ•°è°ƒä¼˜**: ä½¿ç”¨Hydra Sweeper
</details>

---

## ğŸ“š å‚è€ƒæ–‡çŒ®

### æ¨¡å‹è®ºæ–‡

- **ConvNeXt**: [A ConvNet for the 2020s](https://arxiv.org/abs/2201.03545) (CVPR 2022)
- **EfficientNetV2**: [Smaller Models and Faster Training](https://arxiv.org/abs/2104.00298) (ICML 2021)
- **Swin Transformer**: [Hierarchical Vision Transformer using Shifted Windows](https://arxiv.org/abs/2103.14030) (ICCV 2021)

### æŠ€æœ¯æ¡†æ¶

- [PyTorch](https://pytorch.org/) - æ·±åº¦å­¦ä¹ æ¡†æ¶
- [timm](https://github.com/huggingface/pytorch-image-models) - PyTorch Image Models
- [Albumentations](https://github.com/albumentations-team/albumentations) - å›¾åƒå¢å¼ºåº“
- [Hydra](https://hydra.cc/) - é…ç½®ç®¡ç†æ¡†æ¶

---

## ğŸ¤ å‚ä¸è´¡çŒ®

æ¬¢è¿è´¡çŒ®ä»£ç ã€æŠ¥å‘Šé—®é¢˜æˆ–æå‡ºæ”¹è¿›å»ºè®®ï¼

### è´¡çŒ®æµç¨‹

1. Forkæœ¬ä»“åº“
2. åˆ›å»ºç‰¹æ€§åˆ†æ”¯ (`git checkout -b feature/AmazingFeature`)
3. æäº¤æ›´æ”¹ (`git commit -m 'Add some AmazingFeature'`)
4. æ¨é€åˆ°åˆ†æ”¯ (`git push origin feature/AmazingFeature`)
5. å¼€å¯Pull Request

### å¼€å‘è§„èŒƒ

- éµå¾ªPEP 8ä»£ç é£æ ¼
- æ·»åŠ å¿…è¦çš„æ³¨é‡Šå’Œæ–‡æ¡£
- æ›´æ–°ç›¸å…³æ–‡æ¡£
- ç¡®ä¿æ‰€æœ‰æµ‹è¯•é€šè¿‡

---

## ğŸ“„ å¼€æºåè®®

æœ¬é¡¹ç›®é‡‡ç”¨ [MIT License](LICENSE) å¼€æºåè®®ã€‚

---

## ğŸŒŸ è‡´è°¢

- æ„Ÿè°¢ [timm](https://github.com/huggingface/pytorch-image-models) æä¾›çš„ä¼˜ç§€é¢„è®­ç»ƒæ¨¡å‹
- æ„Ÿè°¢ [Albumentations](https://github.com/albumentations-team/albumentations) å›¢é˜Ÿçš„é«˜æ•ˆå¢å¼ºåº“
- æ„Ÿè°¢å…¨å›½é«˜æ ¡è®¡ç®—æœºèƒ½åŠ›æŒ‘æˆ˜èµ›ç»„å§”ä¼šæä¾›çš„æ¯”èµ›å¹³å°

---

## ğŸ“® è”ç³»æ–¹å¼

- **ä½œè€…**: ironhxs
- **GitHub**: [@ironhxs](https://github.com/ironhxs)
- **é¡¹ç›®åœ°å€**: [FlowerRecognition](https://github.com/ironhxs/FlowerRecognition)

å¦‚æœ‰é—®é¢˜æˆ–å»ºè®®ï¼Œæ¬¢è¿æIssueæˆ–å‘é€é‚®ä»¶ï¼

---

<div align="center">

**â­ å¦‚æœè¿™ä¸ªé¡¹ç›®å¯¹ä½ æœ‰å¸®åŠ©ï¼Œè¯·ç»™ä¸ªStaræ”¯æŒä¸€ä¸‹ï¼â­**

Made with â¤ï¸ by ironhxs

ğŸŒ¸ Happy Coding! ğŸŒ¸

</div>
