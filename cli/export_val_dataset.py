"""
å¯¼å‡ºéªŒè¯é›†æ•°æ®

è¿™ä¸ªè„šæœ¬ä¼šï¼š
1. ä½¿ç”¨å’Œè®­ç»ƒæ—¶ç›¸åŒçš„ seed å’Œ val_split å‚æ•°
2. ç”Ÿæˆ val.csv (åŒ…å« image_id å’Œ label)
3. å¯é€‰ï¼šå¤åˆ¶éªŒè¯é›†å›¾ç‰‡åˆ°ç‹¬ç«‹ç›®å½•
"""

import os
import sys
from pathlib import Path
import pandas as pd
import shutil
from sklearn.model_selection import train_test_split
from tqdm import tqdm
import hydra
from omegaconf import DictConfig

# Add parent directory to path
sys.path.append(str(Path(__file__).parent))


@hydra.main(version_base=None, config_path="configs", config_name="swin_v2_anti_overfit")
def export_val_dataset(cfg: DictConfig):
    """å¯¼å‡ºéªŒè¯é›†æ•°æ®"""
    
    print("=" * 70)
    print("ğŸ“¦ å¯¼å‡ºéªŒè¯é›†æ•°æ®")
    print("=" * 70)
    print()
    
    # è¯»å–è®­ç»ƒæ ‡ç­¾æ–‡ä»¶
    train_csv = cfg.dataset.train_csv
    if not os.path.exists(train_csv):
        print(f"âŒ é”™è¯¯: æ‰¾ä¸åˆ°è®­ç»ƒæ ‡ç­¾æ–‡ä»¶ {train_csv}")
        return
    
    df = pd.read_csv(train_csv)
    print(f"âœ… åŠ è½½è®­ç»ƒæ•°æ®: {len(df)} ä¸ªæ ·æœ¬")
    print(f"   åˆ—å: {list(df.columns)}")
    print()
    
    # ä½¿ç”¨å’Œè®­ç»ƒæ—¶ç›¸åŒçš„å‚æ•°è¿›è¡Œåˆ’åˆ†
    val_split = cfg.dataset.val_split
    seed = cfg.seed
    
    print(f"ğŸ“Š åˆ’åˆ†å‚æ•°:")
    print(f"   Val Split: {val_split} ({val_split*100:.1f}%)")
    print(f"   Random Seed: {seed}")
    print()
    
    # åˆ†å±‚åˆ’åˆ†
    train_ids, val_ids, train_labels, val_labels = train_test_split(
        df['image_id'].tolist(),
        df['label'].tolist(),
        test_size=val_split,
        random_state=seed,
        stratify=df['label'].tolist()
    )
    
    print(f"âœ… åˆ’åˆ†å®Œæˆ:")
    print(f"   Train Set: {len(train_ids)} ä¸ªæ ·æœ¬")
    print(f"   Val Set:   {len(val_ids)} ä¸ªæ ·æœ¬")
    print()
    
    # åˆ›å»ºéªŒè¯é›† DataFrame
    val_df = pd.DataFrame({
        'image_id': val_ids,
        'label': val_labels
    })
    
    # æŒ‰ image_id æ’åºï¼ˆä¾¿äºæŸ¥çœ‹ï¼‰
    val_df = val_df.sort_values('image_id').reset_index(drop=True)
    
    # è¾“å‡ºç›®å½•
    output_dir = Path("./exported_val_dataset")
    output_dir.mkdir(exist_ok=True)
    
    # ä¿å­˜ val.csv
    val_csv_path = output_dir / "val.csv"
    val_df.to_csv(val_csv_path, index=False)
    print(f"âœ… ä¿å­˜éªŒè¯é›†æ ‡ç­¾: {val_csv_path}")
    print(f"   æ ·æœ¬æ•°: {len(val_df)}")
    print(f"   ç±»åˆ«åˆ†å¸ƒç»Ÿè®¡:")
    label_counts = val_df['label'].value_counts().sort_index()
    print(f"   - æœ€å°ç±»åˆ«æ ·æœ¬æ•°: {label_counts.min()}")
    print(f"   - æœ€å¤§ç±»åˆ«æ ·æœ¬æ•°: {label_counts.max()}")
    print(f"   - å¹³å‡æ¯ç±»æ ·æœ¬æ•°: {label_counts.mean():.1f}")
    print()
    
    # è¯¢é—®æ˜¯å¦å¤åˆ¶å›¾ç‰‡
    print("â“ æ˜¯å¦å¤åˆ¶éªŒè¯é›†å›¾ç‰‡åˆ°ç‹¬ç«‹ç›®å½•ï¼Ÿ")
    print("   (è¿™ä¼šå ç”¨é¢å¤–ç£ç›˜ç©ºé—´ï¼Œä½†æ–¹ä¾¿è¿ç§»åˆ°å…¶ä»–æœåŠ¡å™¨)")
    copy_images = input("   è¾“å…¥ 'y' å¤åˆ¶å›¾ç‰‡ï¼Œå…¶ä»–é”®è·³è¿‡: ").strip().lower()
    
    if copy_images == 'y':
        # åˆ›å»ºéªŒè¯é›†å›¾ç‰‡ç›®å½•
        val_images_dir = output_dir / "val_images"
        val_images_dir.mkdir(exist_ok=True)
        
        train_dir = Path(cfg.dataset.train_dir)
        
        print()
        print(f"ğŸ“ å¤åˆ¶éªŒè¯é›†å›¾ç‰‡...")
        copied = 0
        missing = 0
        
        for image_id in tqdm(val_ids, desc="å¤åˆ¶ä¸­"):
            src_path = train_dir / image_id
            
            if src_path.exists():
                dst_path = val_images_dir / image_id
                shutil.copy2(src_path, dst_path)
                copied += 1
            else:
                missing += 1
                print(f"   âš ï¸  æ‰¾ä¸åˆ°å›¾ç‰‡: {image_id}")
        
        print()
        print(f"âœ… å›¾ç‰‡å¤åˆ¶å®Œæˆ:")
        print(f"   æˆåŠŸ: {copied} å¼ ")
        if missing > 0:
            print(f"   âš ï¸  ç¼ºå¤±: {missing} å¼ ")
        print(f"   ç›®å½•: {val_images_dir}")
        print()
    
    # ç”Ÿæˆå…ƒæ•°æ®æ–‡ä»¶
    metadata_path = output_dir / "README.txt"
    with open(metadata_path, 'w', encoding='utf-8') as f:
        f.write("éªŒè¯é›†å¯¼å‡ºä¿¡æ¯\n")
        f.write("=" * 70 + "\n\n")
        f.write(f"å¯¼å‡ºæ—¶é—´: {pd.Timestamp.now()}\n")
        f.write(f"é…ç½®æ–‡ä»¶: swin_v2_anti_overfit.yaml\n\n")
        f.write(f"åˆ’åˆ†å‚æ•°:\n")
        f.write(f"  - Val Split: {val_split} ({val_split*100:.1f}%)\n")
        f.write(f"  - Random Seed: {seed}\n")
        f.write(f"  - åˆ†å±‚é‡‡æ ·: æ˜¯ (stratify by label)\n\n")
        f.write(f"æ•°æ®ç»Ÿè®¡:\n")
        f.write(f"  - æ€»æ ·æœ¬æ•°: {len(val_df)}\n")
        f.write(f"  - ç±»åˆ«æ•°: {val_df['label'].nunique()}\n")
        f.write(f"  - æœ€å°ç±»åˆ«æ ·æœ¬æ•°: {label_counts.min()}\n")
        f.write(f"  - æœ€å¤§ç±»åˆ«æ ·æœ¬æ•°: {label_counts.max()}\n")
        f.write(f"  - å¹³å‡æ¯ç±»æ ·æœ¬æ•°: {label_counts.mean():.1f}\n\n")
        f.write(f"æ–‡ä»¶åˆ—è¡¨:\n")
        f.write(f"  - val.csv: éªŒè¯é›†æ ‡ç­¾æ–‡ä»¶ (image_id, label)\n")
        if copy_images == 'y':
            f.write(f"  - val_images/: éªŒè¯é›†å›¾ç‰‡ç›®å½•\n")
        f.write("\n")
        f.write("ä½¿ç”¨æ–¹æ³•:\n")
        f.write("  1. å°†æ•´ä¸ª exported_val_dataset ç›®å½•å¤åˆ¶åˆ°ç›®æ ‡æœåŠ¡å™¨\n")
        f.write("  2. åœ¨ç›®æ ‡æœåŠ¡å™¨ä¸Šè¿è¡Œæ¨ç†éªŒè¯å‡†ç¡®ç‡\n")
        f.write("  3. å¯¹æ¯”ä¸åŒç¯å¢ƒçš„ç»“æœå·®å¼‚\n")
    
    print(f"âœ… ä¿å­˜å…ƒæ•°æ®: {metadata_path}")
    print()
    
    # æ‰“åŒ…å»ºè®®
    print("=" * 70)
    print("ğŸ‰ å¯¼å‡ºå®Œæˆï¼")
    print("=" * 70)
    print()
    print("ğŸ“¦ æ‰“åŒ…å‘½ä»¤ (ç”¨äºä¼ è¾“åˆ°å…¶ä»–æœåŠ¡å™¨):")
    print(f"   cd {output_dir.parent}")
    print(f"   tar -czf val_dataset.tar.gz {output_dir.name}/")
    print()
    print("ğŸ“¤ ä¼ è¾“åˆ°ç›®æ ‡æœåŠ¡å™¨:")
    print(f"   scp val_dataset.tar.gz user@server:/path/to/destination/")
    print()
    print("ğŸ“‚ è§£å‹:")
    print(f"   tar -xzf val_dataset.tar.gz")
    print()
    print("=" * 70)


if __name__ == "__main__":
    export_val_dataset()
